# Gradient-Descent-and-its-variants
Gradient Descent Gradient Descent is a very common optimization algorithm, and most likely, the first optimization algorithm a lot of Machine learning engineers and Data scientists are introduced to. 
For an example, consider we have a cost function, and we need to find the optimum solution to solve the cost function. Here comes gradient descent, an algorithm that works by making changes to the values of the parameters that are within the model, all in the purpose of minimizing the cost function. 
An example of a cost function is Mean Squared Error. Gradient descent intrinsic functionality works by finding the direction to take towards a local minimum based on the calculated gradient obtained from the error function with respect to the parameters at a particular data point.


